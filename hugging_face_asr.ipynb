{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d878d6-560e-43e9-be23-ccf6bd6d8784",
   "metadata": {},
   "source": [
    "# Finetuning Hugging Face Wav2Vec2 model on LibriSpeech dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4533bc2-de6b-420c-ab79-20690824b9f8",
   "metadata": {},
   "source": [
    "## Init HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0be56-c169-44d0-9ee9-0ab1a8398863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bb2d0-268e-4511-ab2b-81086dad68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5de7f-8c9c-4898-bd57-29e641e6ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"wav2vec2-finetuning-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036afa8-2162-47ad-aa9c-f84f9800e55a",
   "metadata": {},
   "source": [
    "## LibriSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3994c8-cbf5-4ca2-ad66-1fd638402444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b2806-d7a4-4f35-a061-9569b22e4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"train\": torchaudio.datasets.LIBRISPEECH(root=\"\", url=\"train-clean-100\"), \n",
    "    \"val\": torchaudio.datasets.LIBRISPEECH(root=\"\", url=\"dev-clean\"), \n",
    "    \"test\": torchaudio.datasets.LIBRISPEECH(root=\"\", url=\"test-clean\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fa722-63d0-44f5-b6d3-a03006cfa836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(sample):\n",
    "    audio = sample[0][0]\n",
    "    sampling_rate = sample[1]\n",
    "    text = sample[2].lower()\n",
    "    \n",
    "    return {\n",
    "        \"audio\": sample[0][0], \n",
    "        \"sampling_rate\": sample[1], \n",
    "        \"text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f15da-f089-4000-89fe-285642216859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(transform_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77918270-ab7d-4b80-a98f-95ebebe35024",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66631d-7030-45f6-9173-97c581d577f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04397723-25a3-4536-ac0c-f5a4bdeff436",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {w: idx for idx, w in enumerate(string.ascii_lowercase)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d054f-c154-41c8-b0cd-12a32f40ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.update({\n",
    "    \"|\": len(vocab), \n",
    "    \"'\": len(vocab) + 1, \n",
    "    \"<UNK>\": len(vocab) + 2, \n",
    "    \"<PAD>\": len(vocab) + 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b7ee1-b480-4a88-a6aa-d44931a908f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcdb13-def4-4458-8212-11e27a7dce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31ec77-9588-4ee5-9ded-cfa8773ef083",
   "metadata": {},
   "source": [
    "## Text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7f518-f6dc-4dec-8b57-bd072da215e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f486a34-d7d4-40ce-93b8-1e98c6647533",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"./vocab.json\", unk_token=\"<UNK>\", pad_token=\"<PAD>\", word_delimiter_token=\"|\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb82d5c-182f-44b8-99cf-f38f1c73e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dfb86-6f33-4275-9486-23aff2ca2efa",
   "metadata": {},
   "source": [
    "## Audio Wav2Vec2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad4b33-72f7-4354-a267-f56cc075dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor, \n",
    "    Wav2Vec2Processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1dfa2d-b575-44b1-9c68-1af4caf83729",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, sampling_rate=16000, padding_value=0.0, \n",
    "    do_normalize=True, return_attntion_mask=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085a508-dd14-42d8-8716-84afc748191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99713f51-3ebb-4f2d-b9d6-8d4a3ff5833d",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665115f6-1681-448b-8bdc-a6cda453942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(sample):\n",
    "    sample['audio'] = processor(sample['audio'], sampling_rate=sample[\"sampling_rate\"]).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        sample['label'] = processor(sample['text']).input_ids\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922728-7f87-43d8-99e2-0978a32e13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(data_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f89769-dbec-425c-bda4-30a6937b4cb0",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2be9d4-7eff-4f83-8437-d2df86183f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58c577-1f8f-469a-928f-20d81e927316",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        input_features = [{'input_values': s['audio']} for s in samples]\n",
    "        label_features = [{'input_ids': s['label']} for s in samples]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features, \n",
    "            padding=self.padding, \n",
    "            max_length=self.max_length, \n",
    "            pad_to_multiple_of=self.pad_to_multiple_of, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features, \n",
    "                padding=self.padding, \n",
    "                max_length=self.max_length_labels, \n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch['labels'] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51150db-1918-4a37-ba9c-70d0ec00689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbc770-26e2-4025-86b7-811cd49cf88b",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f178f-a9ea-4a3a-b34a-e692c6e42a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ecd42-26c5-46ae-94f3-1698f81c049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric('wer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71680817-f9d0-4c20-aec4-140080a478a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {'wer': wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7bd90-a91d-4908-ac22-559ddf1646ca",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9e99a-c7c3-4d7e-abcd-f93dd8a86f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1bbe43-98db-49d6-b5a1-96f9f14e3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    'facebook/wav2vec2-base', \n",
    "    ctc_loss_reduction='mean', \n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c2466-a719-449a-8be7-dd146dc8c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3adc90-36fb-4f1b-9f74-8e29276af017",
   "metadata": {},
   "source": [
    "## Init arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c10d5-b6c3-4ebc-857b-1fade22af8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f566774-ace4-4132-9a19-9eb4f0c6bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_name, \n",
    "    group_by_length=True, \n",
    "    per_device_train_batch_size=32, \n",
    "    evaluation_strategy='steps', \n",
    "    num_train_epochs=30, \n",
    "    fp16=True, \n",
    "    gradient_checkpointing=True, \n",
    "    save_steps=500, \n",
    "    eval_steps=500, \n",
    "    logging_steps=500, \n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=5e-3, \n",
    "    warmup_steps=1000, \n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667e4a4-8325-4e0e-bd86-896fef65ace3",
   "metadata": {},
   "source": [
    "## Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad370ccc-03ef-4e04-bda3-f1973c01467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07c9ae-1b01-44cf-9e64-0ec9f98f6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    data_collator=data_collator, \n",
    "    args=training_args, \n",
    "    compute_metrics=compute_metrics, \n",
    "    train_dataset=timit_prepared['train'], \n",
    "    eval_dataset=timit_prepared['test'], \n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9e17b-b5d7-43d7-8473-23e248e430be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515335c-9a09-494d-bff7-dffd254027cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
