{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e59b7b-da04-4c7f-9f99-32f3a76cf572",
   "metadata": {},
   "source": [
    "# QuartzNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f501668-618a-4b81-b1ca-798c65984054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0df75f-8d69-45fe-a854-8f6dc83e46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ccbc4-b9b8-4bd4-a46b-3d05b5f884c3",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b4d93d2-559c-4e49-b38b-23bf3acbb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRProcessor(object):\n",
    "    def __init__(self, sampling_rate, n_fft=1024, hop_length=512, n_mels=128):\n",
    "        self.audio_preprocessor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        self.let2idx = [s for s in string.ascii_lowercase]\n",
    "        self.let2idx.extend([\" \", \"'\", \"<UNK>\", \"<PAD>\"])\n",
    "        self.vocab = {w: idx for idx, w in enumerate(self.let2idx)}\n",
    "\n",
    "    def text_preprocessor(self, text):\n",
    "        sym_tokenize = [s for s in text]\n",
    "        \n",
    "        return torch.LongTensor([self.vocab[symbol] for symbol in sym_tokenize])\n",
    "    \n",
    "    def __call__(self, input_values: torch.tensor, labels: str) -> Dict[str, Union[torch.tensor, torch.LongTensor]]:\n",
    "        return {\n",
    "            'input_features': self.audio_preprocessor(input_values), \n",
    "            'labels': self.text_preprocessor(labels)\n",
    "        }\n",
    "    \n",
    "    def labels_decode(self, labels):\n",
    "        return ''.join([self.let2idx[l] for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4c3e-3638-4407-ae17-5a73c513391c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "58b379dd-8d43-4d5a-a2c1-81668acd9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, root='', split='val', max_length=150):\n",
    "        assert split in ['dev-clean', 'dev-other', 'test-clean', 'test-other', 'train-clean-100'], \\\n",
    "                'Split error!'\n",
    "        \n",
    "        self.data_iterator = torchaudio.datasets.LIBRISPEECH(root=root, url=split)\n",
    "        \n",
    "        self.processor = ASRProcessor(sampling_rate=16000)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_iterator[idx]\n",
    "        sample = self.processor(sample[0][0], sample[2].lower())\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_collate(batch):\n",
    "        # Collate audio samples.  \n",
    "        batch_tokens_lengths = torch.tensor([x['input_features'].size(1) for x in batch])\n",
    "        max_len_per_batch = torch.max(batch_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        max_len_per_batch += (max_len_per_batch % 2)\n",
    "        lengths_to_pad = max_len_per_batch - batch_tokens_lengths - 1\n",
    "        \n",
    "        input_features = torch.stack([\n",
    "            F.pad(x['input_features'], pad=(0, val_to_pad)) \n",
    "            for x, val_to_pad in zip(batch, lengths_to_pad)\n",
    "        ])\n",
    "        \n",
    "        # Collate label samples.  \n",
    "        batch_tokens_lengths = torch.tensor([len(x['labels']) for x in batch])\n",
    "        max_len_per_batch = torch.max(batch_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        max_len_per_batch += (max_len_per_batch % 2)\n",
    "        lengths_to_pad = max_len_per_batch - batch_tokens_lengths - 1\n",
    "\n",
    "        labels = torch.vstack([\n",
    "            F.pad(x['labels'], pad=(0, val_to_pad), value=29) \n",
    "            for x, val_to_pad in zip(batch, lengths_to_pad)\n",
    "        ]).type(torch.int64)\n",
    "\n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    def labels_decode(self, labels):\n",
    "        return self.processor.labels_decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5651554b-e5aa-485b-8508-1008f92de7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': LibriDataset(split='train-clean-100'), \n",
    "    'val': LibriDataset(split='dev-clean')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "84d273c1-5731-4377-a376-b2b3477deb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    k: DataLoader(dataset[k], batch_size=BATCH_SIZE, shuffle=False, \n",
    "                    collate_fn=LibriDataset.batch_collate, num_workers=1)\n",
    "    for k in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890add-9628-428c-a401-a0fac9e8e2f7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816a157-69e4-4723-bb54-ddb0d7f3e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
