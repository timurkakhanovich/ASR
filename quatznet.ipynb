{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e59b7b-da04-4c7f-9f99-32f3a76cf572",
   "metadata": {},
   "source": [
    "# QuartzNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f501668-618a-4b81-b1ca-798c65984054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f0df75f-8d69-45fe-a854-8f6dc83e46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ccbc4-b9b8-4bd4-a46b-3d05b5f884c3",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b4d93d2-559c-4e49-b38b-23bf3acbb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRProcessor(object):\n",
    "    def __init__(self, sampling_rate=16000, n_fft=1024, hop_length=256, n_mels=64):\n",
    "        self.audio_preprocessor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        self.let2idx = [\" \", \"'\", \"<PAD>\"]\n",
    "        self.let2idx.extend([s for s in string.ascii_lowercase])\n",
    "        self.vocab = {w: idx for idx, w in enumerate(self.let2idx)}\n",
    "\n",
    "    def text_preprocessor(self, text):\n",
    "        sym_tokenize = [s for s in text]\n",
    "        \n",
    "        return torch.LongTensor([self.vocab[symbol] for symbol in sym_tokenize])\n",
    "    \n",
    "    def __call__(self, input_values: torch.tensor, labels: str) -> Dict[str, Union[torch.tensor, torch.LongTensor]]:\n",
    "        return {\n",
    "            'input_features': self.audio_preprocessor(input_values), \n",
    "            'labels': self.text_preprocessor(labels)\n",
    "        }\n",
    "    \n",
    "    def labels_decode(self, batched_labels):\n",
    "        pad_idx = len(processor.vocab) - 1\n",
    "        space_idx = len(processor.vocab) - 2\n",
    "        decoding_labels = batched_labels.clone().detach()\n",
    "        \n",
    "        decoding_labels[decoding_labels == pad_idx] = space_idx\n",
    "        return [''.join([self.let2idx[l] for l in bl]).strip() for bl in decoding_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4c3e-3638-4407-ae17-5a73c513391c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "58b379dd-8d43-4d5a-a2c1-81668acd9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, processor, root='', split='val', max_length=150):\n",
    "        assert split in ['dev-clean', 'dev-other', 'test-clean', 'test-other', 'train-clean-100'], \\\n",
    "                'Split error!'\n",
    "        \n",
    "        self.data_iterator = torchaudio.datasets.LIBRISPEECH(root=root, url=split)\n",
    "        \n",
    "        self.processor = processor\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_iterator[idx]\n",
    "        sample = self.processor(sample[0][0], sample[2].lower())\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_collate(batch):\n",
    "        # Collate audio samples.  \n",
    "        sample_tokens_lengths = torch.tensor([x['input_features'].size(1) for x in batch])\n",
    "        max_len_per_samples = torch.max(sample_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_samples % 2)\n",
    "        max_len_per_samples += additive\n",
    "        samples_lengths_to_pad = max_len_per_samples - sample_tokens_lengths - additive\n",
    "        \n",
    "        input_features = torch.stack([\n",
    "            F.pad(x['input_features'], pad=(0, val_to_pad)) \n",
    "            for x, val_to_pad in zip(batch, samples_lengths_to_pad)\n",
    "        ])\n",
    "        \n",
    "        # Collate label samples.  \n",
    "        label_tokens_lengths = torch.tensor([x['labels'].size(0) for x in batch])\n",
    "        max_len_per_labels = torch.max(label_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_labels % 2)\n",
    "        max_len_per_labels += additive\n",
    "        labels_lengths_to_pad = max_len_per_labels - label_tokens_lengths - additive\n",
    "\n",
    "        labels = torch.vstack([\n",
    "            F.pad(x['labels'], pad=(0, val_to_pad), value=len(processor.vocab) - 1) \n",
    "            for x, val_to_pad in zip(batch, labels_lengths_to_pad)\n",
    "        ]).type(torch.int64)\n",
    "\n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'input_lengths': torch.ceil(sample_tokens_lengths / 2).type(torch.int64), \n",
    "            'targets': labels, \n",
    "            'target_lengths': label_tokens_lengths\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2eed184f-591e-4aca-8b9e-cb1ae984d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ASRProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5651554b-e5aa-485b-8508-1008f92de7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': LibriDataset(processor, split='train-clean-100'), \n",
    "    'val': LibriDataset(processor, split='dev-clean')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "84d273c1-5731-4377-a376-b2b3477deb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    k: DataLoader(dataset[k], batch_size=BATCH_SIZE, shuffle=False, \n",
    "                    collate_fn=LibriDataset.batch_collate, num_workers=0)\n",
    "    for k in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890add-9628-428c-a401-a0fac9e8e2f7",
   "metadata": {},
   "source": [
    "## Model (QuartzNet 5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d2e5e8ae-7a85-41da-aeff-a6eeb1cc5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, activation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Padding 'same'.  \n",
    "        padding = (kernel_size // 2) * dilation\n",
    "        \n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, kernel_size, stride, \n",
    "            padding=padding, dilation=dilation, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        TCS_out = self.pointwise(self.depthwise(x))\n",
    "        \n",
    "        bn_out = self.batch_norm(TCS_out)\n",
    "        \n",
    "        return F.relu(bn_out) if self.activation else bn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "542c83a9-0a60-420d-b973-8aab23e4f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedBBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, R=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The first block to match in_channels and out_channels\n",
    "        self.B = [SingleBBlock(in_channels, out_channels, kernel_size, stride, dilation, activation=True)]\n",
    "        \n",
    "        # BBlocks between the first and the last blocks.  \n",
    "        self.B.extend([\n",
    "            SingleBBlock(out_channels, out_channels, kernel_size, stride, dilation, activation=True)\n",
    "            for _ in range(R - 2)\n",
    "        ])\n",
    "        \n",
    "        # The last block to prevent nonlinearity.  \n",
    "        self.B.append(SingleBBlock(out_channels, out_channels, kernel_size, stride, dilation, activation=False))\n",
    "        self.B = nn.Sequential(*self.B)\n",
    "        \n",
    "        # Skip connection.  \n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1), \n",
    "            nn.BatchNorm1d(num_features=out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        RBlocks_out = self.B(x)\n",
    "        skip_out = self.skip_connection(x)\n",
    "        \n",
    "        return F.relu(RBlocks_out + skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d816a157-69e4-4723-bb54-ddb0d7f3e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C1 = SingleBBlock(\n",
    "            in_channels=n_features, out_channels=256, kernel_size=33, \n",
    "            stride=2, dilation=1, activation=True\n",
    "        )\n",
    "        \n",
    "        self.B = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('B1', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=33, stride=1, dilation=1, R=5)), \n",
    "                ('B2', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=39, stride=1, dilation=1, R=5)), \n",
    "                ('B3', RepeatedBBlocks(in_channels=256, out_channels=512, kernel_size=51, stride=1, dilation=1, R=5)), \n",
    "                ('B4', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=63, stride=1, dilation=1, R=5)), \n",
    "                ('B5', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=75, stride=1, dilation=1, R=5))\n",
    "            ])\n",
    "        )\n",
    "        self.C2 = SingleBBlock(\n",
    "            in_channels=512, out_channels=512, kernel_size=87, stride=1, dilation=2, activation=True\n",
    "        )\n",
    "        \n",
    "        self.C3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=1, stride=1, dilation=1), \n",
    "            nn.BatchNorm1d(num_features=1024), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.C4 = nn.Conv1d(in_channels=1024, out_channels=n_classes, kernel_size=1, stride=1, dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_conv_out = self.C1(x)\n",
    "\n",
    "        b_out = self.B(first_conv_out)\n",
    "        c2_out = self.C2(b_out)\n",
    "        c3_out = self.C3(c2_out)\n",
    "        c4_out = self.C4(c3_out)\n",
    "        \n",
    "        return c4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fa4bec29-1f3c-42de-817f-03f295072510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_it = iter(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c05e2165-6ec5-41b4-9f48-ce3c3cbb31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(data_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6fa73f03-a186-4b77-ab32-eb70ceb1a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([201, 283, 250, 268, 227, 263, 160, 236, 262, 196, 262, 246, 251, 271,\n",
       "        242,  85, 284, 246, 226, 258, 243, 266, 293, 229, 264, 260, 126, 258,\n",
       "        254, 238, 268, 318])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "33aa0de4-1cfe-4312-bf14-2a1aebf3fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuartzNet(n_features=64, n_classes=len(ASRProcessor().vocab) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5cac12e7-d97d-412c-a770-4acb65e02876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(sample['input_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "99404d16-afd4-40c8-8caa-83f22972de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 527])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "36c0028b-2f18-41f7-b088-9b7a9c346431",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1ee4180c-7732-47f2-8de4-8066c3f221f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 527])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "491b7a0d-b158-425c-9a1c-39dfa050dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 527])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767802-7f4f-4a9a-b4b7-f32cdd8ebc3c",
   "metadata": {},
   "source": [
    "## Metrics and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3cfa558d-789b-40b1-8f57-219accc7deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wer_metric = load_metric('wer')\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward of labels prediction:\n",
    "        :param y_pred: log_softmax(output) of the model, shape: (T, B, C), \n",
    "        :param y_true: true labels, shape (B, T)\n",
    "        \"\"\"\n",
    "        pred_ids = torch.argmax(y_pred, dim=2).T\n",
    "    \n",
    "        pred_str = processor.labels_decode(pred_ids)\n",
    "        label_str = processor.labels_decode(y_true)\n",
    "\n",
    "        wer = self.wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "        return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "07300da8-28ca-42bf-8195-2c1749e3d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_dataloader, criterion, metrics, \n",
    "                    device=torch.device('cpu'), return_train=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        print(\"\\n\")\n",
    "        for batch_idx, sample in enumerate(val_dataloader):\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(val_dataloader) - 1:\n",
    "                print(f\"==> Batch: {batch_idx}/{len(val_dataloader)}\")\n",
    "            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_score += metrics(sample['log_probs'], sample['targets'])\n",
    "\n",
    "        running_loss /= len(val_dataloader)\n",
    "        running_score /= len(val_dataloader)\n",
    "        \n",
    "    if return_train:\n",
    "        model.train()\n",
    "\n",
    "    return running_loss, running_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259e329-dece-444d-a5a9-4d3b118e1bc6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7572d115-19db-4c84-b0f6-cdc04c1c54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1495fc5b-e82f-40f9-bdd1-13086658489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, criterion, optimizer, metrics, scheduler=None, \n",
    "          num_epochs=5, start_epoch=-1, prev_metrics=dict(), device=torch.device('cpu'),\n",
    "          folder_for_checkpoints='/'):\n",
    "    for key, vals in prev_metrics.items():\n",
    "        for val in vals:\n",
    "            mlflow.log_metric(key, val[1])\n",
    "\n",
    "    if len(prev_metrics) > 0:\n",
    "        history = copy.deepcopy(prev_metrics)\n",
    "        curr_step = prev_metrics['train_loss'][-1][0] + 1\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        curr_step = 1\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch + 1, start_epoch + 1 + num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "\n",
    "        clear_output(True)\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Epoch: {epoch}/{start_epoch + num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(\"Train: \")\n",
    "\n",
    "        for batch_idx, sample in enumerate(tqdm(dataloaders['train'])):            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            running_score += metrics(sample['log_probs'], sample['targets'])\n",
    "\n",
    "            if batch_idx % 1001 == 1000:\n",
    "                val_loss, val_metrics = validate_model(model, dataloaders['val'], criterion, \n",
    "                                                        metrics, device, return_train=True)\n",
    "                \n",
    "                mlflow.log_metric('val_loss', val_loss)\n",
    "                mlflow.log_metric('train_loss', running_loss / (batch_idx + 1))\n",
    "\n",
    "                mlflow.log_metric('val_wer', val_metrics)\n",
    "                history['val_wer'].append((curr_step, val_metrics))\n",
    "                \n",
    "                mlflow.log_metric('train_wer', score / (batch_idx + 1))\n",
    "                history['train_wer'].append((curr_step, score / (batch_idx + 1)))\n",
    "                \n",
    "                history['train_loss'].append((curr_step, running_loss / (batch_idx + 1)))\n",
    "                history['val_loss'].append((curr_step, val_loss))\n",
    "\n",
    "            curr_step += 1\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(\"Validate: \")\n",
    "        mean_val_loss, mean_val_score = validate_model(model, dataloaders['val'], criterion, \n",
    "                                                        metrics, device, return_train=True)\n",
    "\n",
    "        print(f'Mean val loss: {mean_val_loss}')\n",
    "        for metric, score in mean_val_score.items():\n",
    "            print(f'\\nMean {metric}: {score}')\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'batch_size_training': dataloaders['train'].batch_size,\n",
    "            'model_architecture': model,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler': scheduler if scheduler else None,\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'whole_history': history\n",
    "        }\n",
    "\n",
    "        torch.save(state, folder_for_checkpoints + f'checkpoint_epoch_{epoch + 1}.pt')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3f022da5-5eb9-4d6e-b5ff-d9449e48ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss()\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c2394b69-5869-4991-a040-da6cc0f12bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> Batch: 0/85\n",
      "==> Batch: 10/85\n",
      "==> Batch: 20/85\n",
      "==> Batch: 30/85\n",
      "==> Batch: 40/85\n",
      "==> Batch: 50/85\n",
      "==> Batch: 60/85\n",
      "==> Batch: 70/85\n",
      "==> Batch: 80/85\n",
      "==> Batch: 84/85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.55126221039716, 1.0)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model, dataloaders['val'], criterion, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "190b8340-fba3-43e3-b1b9-661d6e27e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0132404181184669"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits, sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f55e1fc6-2f35-4bb8-bee3-2b3a4545174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_for_loss = torch.log(logits).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1a95d25b-480b-405e-96c1-5f72c68661d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527, 32, 28])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_for_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "41fcf23e-8666-46b4-b22b-4fa7c8627ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3625, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(logits_for_loss, sample['labels'], sample['input_lengths'], sample['labels_lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843cc25-8a0c-4873-92bf-f48b1f8582f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
