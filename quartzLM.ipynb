{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc15ef7-2ce2-4c13-83dd-5e9eabd2322c",
   "metadata": {},
   "source": [
    "# QuartzNet + LM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312446a5-afad-495e-88bc-c9dcd1876dfc",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "* Look at the distribution of mel-spec and probably clip values\n",
    "* Simple LSTM seq2seq model\n",
    "* Beam Search\n",
    "* cuda.amp\n",
    "* (optional) BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e4f17a-34a5-4399-b784-d89232588603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "from collections import OrderedDict, defaultdict\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from functools import partial\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from bpemb import BPEmb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "BPEMB_EN = BPEmb(lang=\"en\", dim=300, vs=1000)\n",
    "PAD_IDX = BPEMB_EN.EOS\n",
    "DEVICE = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c8fb4-710f-4c6a-86ef-c82fa11bf3be",
   "metadata": {},
   "source": [
    "## Set seed to all processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b0c3ef-f301-4712-a9d4-0d02fbe584a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4765f330-38b4-4518-a8b9-f7aa64264430",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873d37f-7bf9-45b9-8baa-0983c74f1233",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e72b76-eac0-446a-b193-cd9412c0d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsqueeze(nn.Module):\n",
    "    def __init__(self, dim=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d37fab-64dc-41ba-8f05-563d3913a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRProcessor(object):\n",
    "    def __init__(self, tokenizer, sampling_rate=16000, n_fft=1024, \n",
    "                 hop_length=256, n_mels=64, split='test'):\n",
    "        self.mel_spec_processor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.augmentation = nn.Sequential(\n",
    "                Unsqueeze(), \n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=5), \n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=5), \n",
    "            )\n",
    "        else:\n",
    "            self.augmentation = None\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def text_processor(self, text):\n",
    "        encoded_text = [self.tokenizer.BOS] + \\\n",
    "                       self.tokenizer.encode_ids(text) + \\\n",
    "                       [self.tokenizer.EOS]\n",
    "        \n",
    "        return torch.tensor(encoded_text)\n",
    "    \n",
    "    def __call__(self, input_values: torch.tensor, labels: str) -> Dict[str, Union[torch.tensor, torch.LongTensor]]:\n",
    "        # Got such boundaries for 99.8% of non-augmented train data: [-10.7776, 6.4294].  \n",
    "        log_mel_spec_image = torch.log(self.mel_spec_processor(input_values) + 1e-6).clamp_(-10, 6)\n",
    "        input_preprocessed = self.augmentation(log_mel_spec_image).squeeze(0) \\\n",
    "                                if self.augmentation else log_mel_spec_image\n",
    "        \n",
    "        text_preprocessed = self.text_processor(labels)\n",
    "        \n",
    "        return {\n",
    "            'input_features': input_preprocessed, \n",
    "            'labels': text_preprocessed\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f1329-d841-494f-92a1-4bd4a3035623",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a12725-5fc8-44f0-a693-f1fcf6331f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, processor, root='', split='dev-clean'):\n",
    "        assert split in ['dev-clean', 'dev-other', 'test-clean', 'test-other', 'train-clean-100'], \\\n",
    "                'Split error!'\n",
    "        \n",
    "        self.data_iterator = torchaudio.datasets.LIBRISPEECH(root=root, url=split)\n",
    "        \n",
    "        self.processor = processor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_iterator[idx]\n",
    "        sample = self.processor(sample[0][0], sample[2].lower())\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_batch(sample, input_lengths):\n",
    "        input_features = sample['input_features']\n",
    "        all_means = torch.zeros(input_features.size(0))\n",
    "        all_stds = torch.zeros(input_features.size(0))\n",
    "\n",
    "        for s_idx, s_len in enumerate(input_lengths):\n",
    "            valid_features = input_features[s_idx, :, :s_len]\n",
    "            sample_mean = torch.mean(valid_features)\n",
    "            sample_std = torch.sqrt(torch.mean(valid_features**2) - sample_mean**2)\n",
    "\n",
    "            input_features[s_idx, :, :s_len] = \\\n",
    "                (valid_features - sample_mean) / sample_std\n",
    "        \n",
    "        return input_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_collate(batch, value_to_pad_tokens):\n",
    "        # Collate audio samples.  \n",
    "        sample_tokens_lengths = torch.tensor([x['input_features'].size(1) for x in batch])\n",
    "        max_len_per_samples = torch.max(sample_tokens_lengths)\n",
    "\n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_samples % 2)\n",
    "        max_len_per_samples += additive\n",
    "        samples_lengths_to_pad = max_len_per_samples - sample_tokens_lengths - additive\n",
    "\n",
    "        input_features = torch.stack([\n",
    "            F.pad(x['input_features'], pad=(0, val_to_pad)) \n",
    "            for x, val_to_pad in zip(batch, samples_lengths_to_pad)\n",
    "        ])\n",
    "\n",
    "        # Collate label samples.  \n",
    "        label_tokens_lengths = torch.tensor([x['labels'].size(0) for x in batch])\n",
    "        max_len_per_labels = torch.max(label_tokens_lengths)\n",
    "\n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_labels % 2)\n",
    "        max_len_per_labels += additive\n",
    "        labels_lengths_to_pad = max_len_per_labels - label_tokens_lengths - additive\n",
    "\n",
    "        labels = torch.vstack([\n",
    "            F.pad(x['labels'], pad=(0, val_to_pad), value=value_to_pad_tokens) \n",
    "            for x, val_to_pad in zip(batch, labels_lengths_to_pad)\n",
    "        ]).type(torch.int64)\n",
    "\n",
    "        collated = {\n",
    "            'input_features': input_features,\n",
    "            'targets': labels,\n",
    "            'attention_mask': (labels != PAD_IDX).type(torch.int32)\n",
    "        }\n",
    "\n",
    "        collated['input_features'] = LibriDataset.normalize_batch(\n",
    "            collated, sample_tokens_lengths\n",
    "        )\n",
    "\n",
    "        return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdbc0fb-8938-4397-ad89-5ba9918df06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Data:\n",
    "    datasets: Dict[str, LibriDataset]\n",
    "    dataloaders: Dict[str, torch.utils.data.dataloader.DataLoader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2444d7c-00fb-4dcb-9eab-9622d270a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_split(batch_size=32, train_shuffle=False, collator=None):\n",
    "    train_processor = ASRProcessor(BPEMB_EN, split='test')\n",
    "    test_processor = ASRProcessor(BPEMB_EN, split='test')\n",
    "    \n",
    "    datasets = {\n",
    "        'train': LibriDataset(train_processor, split='train-clean-100'), \n",
    "        'val': LibriDataset(test_processor, split='dev-clean'), \n",
    "        'test': LibriDataset(test_processor, split='test-clean')\n",
    "    }\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': DataLoader(datasets['train'], batch_size=batch_size, shuffle=train_shuffle, \n",
    "                                collate_fn=collator, num_workers=4), \n",
    "        'val': DataLoader(datasets['val'], batch_size=32, shuffle=False, \n",
    "                            collate_fn=collator, num_workers=4), \n",
    "        'test': DataLoader(datasets['test'], batch_size=32, shuffle=False, \n",
    "                            collate_fn=collator, num_workers=4)\n",
    "    }\n",
    "    \n",
    "    return Data(datasets, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a830033-e7b6-446a-aaa1-5753f8c70629",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b945811-e0e3-46c4-b178-d143e786d61f",
   "metadata": {},
   "source": [
    "### QuartzNet 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e2b14f-cd15-452e-be52-2b9b67a1c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, activation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Padding 'same'.  \n",
    "        padding = (kernel_size // 2) * dilation\n",
    "        \n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, kernel_size, stride, \n",
    "            padding=padding, dilation=dilation, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        TCS_out = self.pointwise(self.depthwise(x))\n",
    "        \n",
    "        bn_out = self.batch_norm(TCS_out)\n",
    "        \n",
    "        return F.relu(bn_out) if self.activation else bn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412387bf-a1c4-4195-a9c2-82bc005d0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedBBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, R=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The first block to match in_channels and out_channels.  \n",
    "        self.B = [\n",
    "            SingleBBlock(\n",
    "                in_channels, out_channels, kernel_size, \n",
    "                stride, dilation, activation=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # BBlocks between the first and the last blocks.  \n",
    "        self.B.extend([\n",
    "            SingleBBlock(\n",
    "                out_channels, out_channels, kernel_size, \n",
    "                stride, dilation, activation=True\n",
    "            )\n",
    "            for _ in range(R - 2)\n",
    "        ])\n",
    "        \n",
    "        # The last block to prevent nonlinearity.  \n",
    "        self.B.append(\n",
    "            SingleBBlock(\n",
    "                out_channels, out_channels, kernel_size, \n",
    "                stride, dilation, activation=False\n",
    "            )\n",
    "        )\n",
    "        self.B = nn.Sequential(*self.B)\n",
    "        \n",
    "        # Skip connection.  \n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1), \n",
    "            nn.BatchNorm1d(num_features=out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        RBlocks_out = self.B(x)\n",
    "        skip_out = self.skip_connection(x)\n",
    "        \n",
    "        return F.relu(RBlocks_out + skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041fe1b8-d04b-48ab-81ea-1d12951dd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C1 = SingleBBlock(\n",
    "            in_channels=in_features, out_channels=256, kernel_size=33, \n",
    "            stride=2, dilation=1, activation=True\n",
    "        )\n",
    "        \n",
    "        self.B = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('B1', RepeatedBBlocks(\n",
    "                    in_channels=256, out_channels=256, kernel_size=33, \n",
    "                    stride=1, dilation=1, R=5\n",
    "                )), \n",
    "                ('B2', RepeatedBBlocks(\n",
    "                    in_channels=256, out_channels=256, kernel_size=39, \n",
    "                    stride=1, dilation=1, R=5\n",
    "                )), \n",
    "                ('B3', RepeatedBBlocks(in_channels=256, out_channels=512, kernel_size=51, \n",
    "                                       stride=1, dilation=1, R=5\n",
    "                )), \n",
    "                ('B4', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=63, \n",
    "                                       stride=1, dilation=1, R=5\n",
    "                )), \n",
    "                ('B5', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=75, \n",
    "                                       stride=1, dilation=1, R=5\n",
    "                ))\n",
    "            ])\n",
    "        )\n",
    "        self.C2 = SingleBBlock(\n",
    "            in_channels=512, out_channels=512, kernel_size=87, \n",
    "            stride=1, dilation=2, activation=True\n",
    "        )\n",
    "        \n",
    "        self.C3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=1, stride=1, dilation=1), \n",
    "            nn.BatchNorm1d(num_features=1024), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.C4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1024, out_channels=out_features, kernel_size=1, stride=1, dilation=1), \n",
    "            nn.BatchNorm1d(num_features=out_features), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_conv_out = self.C1(x)\n",
    "\n",
    "        b_out = self.B(first_conv_out)\n",
    "        c2_out = self.C2(b_out)\n",
    "        c3_out = self.C3(c2_out)\n",
    "        c4_out = self.C4(c3_out)\n",
    "        \n",
    "        return c4_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5173cde-0107-4e96-b269-0cab337b174a",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcf730e-939d-460b-af94-b32ddbab6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "        in_features, \n",
    "        hid_size, \n",
    "        num_layers=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.acoustic_model = QuartzNet(in_features, hid_size)\n",
    "        self.lstm_enc = nn.LSTM(\n",
    "            input_size=hid_size, hidden_size=hid_size, \n",
    "            num_layers=num_layers, batch_first=True, dropout=0.2\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        acoustic_out = self.acoustic_model(x).permute(0, 2, 1)\n",
    "        \n",
    "        # x: [B, SEQ, H]\n",
    "        _, (h, c) = self.lstm_enc(acoustic_out)\n",
    "        \n",
    "        return h[-1], c[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a11c727-2523-4efd-bd9d-0475081fa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        hid_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_out = nn.Embedding(num_embeddings=BPEMB_EN.vs, embedding_dim=BPEMB_EN.dim, \n",
    "                                    padding_idx=PAD_IDX)\n",
    "        self.emb_out.weight.data.copy_(torch.from_numpy(BPEMB_EN.vectors))\n",
    "        \n",
    "        self.lstm_dec = nn.LSTMCell(input_size=BPEMB_EN.dim, hidden_size=hid_size)\n",
    "        self.logits = nn.Linear(in_features=hid_size, out_features=BPEMB_EN.vs)\n",
    "    \n",
    "    def decode_step(self, h_prev, c_prev, cur_token):\n",
    "        emb_target = self.emb_out(cur_token)\n",
    "        \n",
    "        hx, cx = self.lstm_dec(emb_target, (h_prev, c_prev))\n",
    "        logits = F.log_softmax(self.logits(hx), dim=-1)\n",
    "        \n",
    "        return logits, (hx, cx)\n",
    "        \n",
    "    def forward(self, hx, cx, target):\n",
    "        seq_first_target = target.T  # (S, B)\n",
    "        \n",
    "        predictions = []\n",
    "        for curr_token in seq_first_target:\n",
    "            logits, (hx, cx) = self.decode_step(hx, cx, curr_token)\n",
    "            predictions.append(logits)\n",
    "        \n",
    "        return torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4b1a7-3d28-4152-bcd4-db6b55445bd7",
   "metadata": {},
   "source": [
    "### QuartzLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b6d62c-50fc-492b-b5e6-5d3a8d9f5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzLM(nn.Module):\n",
    "    def __init__(self, \n",
    "        in_features, hid_size, num_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(in_features, hid_size, num_layers)\n",
    "        self.decoder = Decoder(hid_size)\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        hx, cx = self.encoder(inp)\n",
    "        pred_sequence = self.decoder(hx, cx, target)\n",
    "        \n",
    "        return pred_sequence.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a64c48-2ecd-4865-b7a2-1ce4ebacfbab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam Search Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4a24fe3-4c51-46cd-9a05-3774214b7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result = defaultdict(list)\n",
    "            \n",
    "for i in range(4):\n",
    "    curr_token = torch.tensor([result['seq'][i, -1]])\n",
    "\n",
    "    pred, states = decoder.decode_step(*result['states'][i], curr_token)\n",
    "\n",
    "    top = torch.topk(-pred, k=4, dim=-1)\n",
    "\n",
    "    temp_result['log_probs'].append(result['log_probs'] + top.values.squeeze())\n",
    "    temp_result['seq'].append(top.indices[0])\n",
    "    temp_result['states'].append(states)  # states = (h, c)\n",
    "\n",
    "temp_result['log_probs'] = torch.stack(temp_result['log_probs'])\n",
    "temp_result['seq'] = concat_seqs(result['seq'], torch.stack(temp_result['seq']))\n",
    "top_ids = get_topk_in_matrix(\n",
    "    temp_result['log_probs'], k=4\n",
    ")\n",
    "result['log_probs'] = temp_result['log_probs'][top_ids]\n",
    "result['seq'] = temp_result['seq'][top_ids]\n",
    "result['states'] = [temp_result['states'][idx]\n",
    "                    for idx in top_ids[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe8972-08a9-4ad9-b655-bdb8a05a6970",
   "metadata": {},
   "source": [
    "## Inference Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ff864f8-38f3-47dc-9fe1-417b9364c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_raw_arrays(old_seq: torch.Tensor, new_seq: torch.Tensor):\n",
    "    '''\n",
    "    Parallel arrays concatenation:\n",
    "    torch.tensor([1, 2])\n",
    "    torch.tensor([5, 4, 3, 10])\n",
    "    \n",
    "    Result:\n",
    "    tensor([[ 1,  2,  5],\n",
    "        [ 1,  2,  4],\n",
    "        [ 1,  2,  3],\n",
    "        [ 1,  2, 10]])\n",
    "    '''\n",
    "    \n",
    "    return torch.stack([\n",
    "        torch.cat((old_seq, j)) for j in new_seq.unsqueeze(-1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "350378ad-1e8c-46e3-808a-f729aa057230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_seqs(old_seq: torch.Tensor, new_seq: torch.Tensor):\n",
    "    '''\n",
    "    Parallel sequences concatenation.\n",
    "    '''\n",
    "    \n",
    "    return torch.stack([\n",
    "        concat_raw_arrays(o, n) for o, n in zip(old_seq, new_seq)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66a9b2dc-6617-421e-ba79-36b56371aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_in_matrix(matrix: torch.Tensor, k: int):\n",
    "    '''\n",
    "    Search top k values in matrix\n",
    "    \n",
    "    Returns: indices of top k values in matrix.\n",
    "    '''\n",
    "    \n",
    "    _, topk_ids = torch.topk(matrix.flatten(), k=4, dim=-1)\n",
    "    \n",
    "    return torch.div(topk_ids, 4, rounding_mode='floor'), topk_ids % 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7d0c092-71f0-4424-96cf-fe958bfb509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_beam_search(sample):\n",
    "    '''\n",
    "    Encode sample (melspec), predict the first token of sentence \n",
    "    after BOS token and return the next hidden states\n",
    "    \n",
    "    Returns: \n",
    "    dict of:\n",
    "    * the top k sequnces,\n",
    "    * the top k of their probabilities,\n",
    "    * their hidden and cell states of LSTM\n",
    "    '''\n",
    "    \n",
    "    result = {\n",
    "        'seq': torch.full([4], BPEMB_EN.BOS, dtype=torch.int64),\n",
    "        'log_probs': torch.zeros(4)\n",
    "    }\n",
    "    init_seq = torch.full([1], BPEMB_EN.BOS, dtype=torch.int64)\n",
    "    encoder = Encoder(in_features=N_MELS, hid_size=300, num_layers=2)\n",
    "    decoder = Decoder(hid_size=300)\n",
    "    \n",
    "    # Get initial state\n",
    "    h0, c0 = encoder(sample)\n",
    "    h0 = h0[0].unsqueeze(0)\n",
    "    c0 = c0[0].unsqueeze(0)\n",
    "    \n",
    "    pred1, (h1, c1) = decoder.decode_step(h0, c0, init_seq)\n",
    "    top1 = torch.topk(-pred1, k=4, dim=-1)\n",
    "    \n",
    "    result['seq'] = torch.vstack((result['seq'], top1.indices[0])).T\n",
    "    result['log_probs'] += top1.values[0]\n",
    "    result['states'] = [(h1, c1)] * 4\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb4b589-f1c6-4a93-99a9-21f7f922bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "class InferenceDecoder:\n",
    "    def __init__(self, \n",
    "        language_model: QuartzLM,\n",
    "        max_len: int = 100\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = language_model.encoder\n",
    "        self.decoder = language_model.decoder\n",
    "        \n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def greedy_decoding(self, sample):\n",
    "        batch_size = sample.size(0)\n",
    "        outputs = [torch.full([batch_size], BPEMB_EN.BOS, \n",
    "                              dtype=torch.int64, device=DEVICE)]\n",
    "        \n",
    "        # Get initial state\n",
    "        hx, cx = self.encoder(sample)\n",
    "        for _ in range(self.max_len):\n",
    "            logits, (hx, cx) = self.decoder.decode_step(hx, cx, outputs[-1])\n",
    "            outputs.append(logits.argmax(dim=-1))\n",
    "            \n",
    "        return torch.stack(outputs, dim=1)\n",
    "    \n",
    "    def start_beam_search(self, h0, c0, beam_size):\n",
    "        '''\n",
    "        Encode sample (melspec), predict the first token of sentence \n",
    "        after BOS token and return the next hidden states\n",
    "\n",
    "        Returns: \n",
    "        dict of:\n",
    "        * the top k sequnces,\n",
    "        * the top k of their probabilities,\n",
    "        * their hidden and cell states of LSTM\n",
    "        '''\n",
    "\n",
    "        result = {\n",
    "            'seq': torch.full([beam_size], BPEMB_EN.BOS, dtype=torch.int64),\n",
    "            'log_probs': torch.zeros(beam_size)\n",
    "        }\n",
    "        init_seq = torch.tensor([BPEMB_EN.BOS], dtype=torch.int64)\n",
    "\n",
    "        pred1, (h1, c1) = self.decoder.decode_step(h0, c0, init_seq)\n",
    "        top1 = torch.topk(pred1, k=beam_size, dim=-1)\n",
    "\n",
    "        result['seq'] = torch.vstack((result['seq'], top1.indices[0])).T\n",
    "        result['log_probs'] -= top1.values[0]\n",
    "        result['states'] = [(h1, c1)] * beam_size\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def beam_search_loop(self, result, beam_size):\n",
    "        # Max len excluding BOS and start_iteration\n",
    "        for _ in range(self.max_len - 2):\n",
    "            temp_result = defaultdict(list)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                curr_token = torch.tensor([result['seq'][i, -1]])\n",
    "\n",
    "                pred, states = self.decoder.decode_step(*result['states'][i], curr_token)\n",
    "\n",
    "                top = torch.topk(pred, k=beam_size, dim=-1)\n",
    "\n",
    "                temp_result['log_probs'].append(result['log_probs'] - top.values.squeeze())\n",
    "                temp_result['seq'].append(top.indices[0])\n",
    "                temp_result['states'].append(states)  # states = (h, c)\n",
    "\n",
    "            temp_result['log_probs'] = torch.stack(temp_result['log_probs'])\n",
    "            temp_result['seq'] = self.concat_seqs(result['seq'], torch.stack(temp_result['seq']))\n",
    "            top_ids = get_topk_in_matrix(\n",
    "                temp_result['log_probs'], k=beam_size\n",
    "            )\n",
    "            result['log_probs'] = temp_result['log_probs'][top_ids]\n",
    "            result['seq'] = temp_result['seq'][top_ids]\n",
    "            result['states'] = [temp_result['states'][idx]\n",
    "                                for idx in top_ids[0]]\n",
    "            \n",
    "        high_p_idx = torch.argmax(result['log_probs'])\n",
    "            \n",
    "        return result['seq'][high_p_idx]\n",
    "    \n",
    "    def beam_search_decoding(self, sample, beam_size=4):\n",
    "        batch_size = sample.size(0)\n",
    "        \n",
    "        # Get initial state\n",
    "        h0, c0 = self.encoder(sample)\n",
    "        predictions = torch.zeros(batch_size, self.max_len, dtype=torch.int64)\n",
    "        for batch_idx in range(batch_size):\n",
    "            hx = h0[batch_idx].unsqueeze(0)\n",
    "            cx = c0[batch_idx].unsqueeze(0)\n",
    "            \n",
    "            result = self.start_beam_search(hx, cx, beam_size)\n",
    "            predictions[batch_idx] = self.beam_search_loop(result, beam_size)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def translate_lines(self, input_lines):\n",
    "        result_str = []\n",
    "        for i in input_lines:\n",
    "            bpe_format_str = \\\n",
    "                ''.join([BPEMB_EN.emb.index_to_key[j] for j in input_lines[i][1:]])\n",
    "            result_str.append(' '.join(bpe_format_str.split('â–')).strip())\n",
    "        \n",
    "        return result_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e77b8-c94c-47b9-a1ea-d9d4cf06bc6f",
   "metadata": {},
   "source": [
    "## Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149a3016-40dd-47e3-8b39-5d9cecabf08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_from_checkpoint(checkpoint_path, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    model = checkpoint['model_architecture'].to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer = checkpoint['optimizer']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    history = checkpoint['whole_history']\n",
    "\n",
    "    if checkpoint['scheduler']:\n",
    "        scheduler = checkpoint['scheduler']\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model, \n",
    "        'optimizer': optimizer, \n",
    "        'scheduler': scheduler if checkpoint['scheduler'] else None,\n",
    "        'history': history,\n",
    "        'epoch': epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f81004-31c5-4a80-9275-f4b5eab99234",
   "metadata": {},
   "source": [
    "## Metrics and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1386c848-5db8-4f65-a6f4-56c5b374124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetricsOut:\n",
    "    pred_str: str\n",
    "    target_str: str\n",
    "    wer: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5133bf9b-ce05-4fb2-9e6b-4bc380e80c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wer_metric = load_metric('wer')\n",
    "        self.processor = ASRProcessor(BPEMB_EN)\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward of labels prediction:\n",
    "        :param y_pred: log_softmax(output) of the model, shape: (T, B, C), \n",
    "        :param y_true: true labels, shape (B, T)\n",
    "        \"\"\"\n",
    "        pred_ids = torch.argmax(y_pred, dim=2).T\n",
    "    \n",
    "        pred_str = self.processor.labels_decode(pred_ids, apply_ctc=True)\n",
    "        label_str = self.processor.labels_decode(y_true, apply_ctc=False)\n",
    "        \n",
    "        wer = self.wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "        return MetricsOut(pred_str, label_str, wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6a76cee-536b-4304-bbfc-160dc4580057",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidateOut:\n",
    "    loss: float\n",
    "    metrics: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e3641b8-bde6-416c-825f-5f1970cbd635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_dataloader, criterion, metrics, \n",
    "                    device=torch.device('cpu'), return_train=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        print(\"\\n\")\n",
    "        for batch_idx, sample in enumerate(val_dataloader):\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(val_dataloader) - 1:\n",
    "                print(f\"==> Batch: {batch_idx}/{len(val_dataloader)}\")\n",
    "            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_score += metrics(sample['log_probs'], sample['targets'])\n",
    "\n",
    "        running_loss /= len(val_dataloader)\n",
    "        running_score /= len(val_dataloader)\n",
    "        \n",
    "    if return_train:\n",
    "        model.train()\n",
    "\n",
    "    return ValidateOut(running_loss, running_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1ce26-2d22-49cf-8386-da1de06a17b1",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "276d5a3c-c193-4e10-a214-9f510bd5f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, criterion, optimizer, metrics, scheduler=None, \n",
    "          num_epochs=5, start_epoch=-1, prev_metrics=dict(), device=torch.device('cpu'),\n",
    "          folder_for_checkpoints='/'):\n",
    "    for key, vals in prev_metrics.items():\n",
    "        for val in vals:\n",
    "            wandb.log({key :val[1]}, step=val[0])\n",
    "\n",
    "    if len(prev_metrics) > 0:\n",
    "        history = copy.deepcopy(prev_metrics)\n",
    "        curr_step = prev_metrics['train_loss'][-1][0] + 1\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        curr_step = 1\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch + 1, start_epoch + 1 + num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "\n",
    "        clear_output(True)\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Epoch: {epoch}/{start_epoch + num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(\"Train: \")\n",
    "\n",
    "        for batch_idx, sample in enumerate(tqdm(dataloaders['train'])):            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            model_out = metrics(sample['log_probs'], sample['targets'])\n",
    "            running_score += model_out.wer\n",
    "\n",
    "        val_result = validate_model(model, dataloaders['val'], criterion, \n",
    "                                    metrics, device, return_train=True)\n",
    "        \n",
    "        val_loss = val_result.loss\n",
    "        val_metrics = val_result.metrics\n",
    "        \n",
    "        wandb.log({'train_loss': running_loss / (batch_idx + 1)}, step=curr_step)\n",
    "        wandb.log('val_loss', val_loss, step=curr_step)\n",
    "        history['train_loss'].append((curr_step, running_loss / (batch_idx + 1)))\n",
    "        history['val_loss'].append((curr_step, val_loss))\n",
    "\n",
    "        wandb.log({'val_wer': val_metrics}, step=curr_step)\n",
    "        history['val_wer'].append((curr_step, val_metrics))\n",
    "\n",
    "        wandb.log({'train_wer': running_score / (batch_idx + 1)}, step=curr_step)\n",
    "        history['train_wer'].append((curr_step, running_score / (batch_idx + 1)))\n",
    "\n",
    "        curr_step += 1\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'batch_size_training': dataloaders['train'].batch_size,\n",
    "            'model_architecture': model,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler': scheduler if scheduler else None,\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'whole_history': history\n",
    "        }\n",
    "\n",
    "        torch.save(state, folder_for_checkpoints + f'checkpoint_epoch_{epoch%5 + 1}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ec377-68ed-417e-8b71-d33754c49cc6",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df1994a1-80b1-42df-8170-07acf444bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSOR = ASRProcessor(BPEMB_EN)\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "CHECKPOINT_PATH = \"ASR_checkpoints/\"\n",
    "N_MELS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96180808-b406-499e-84eb-287f7621258e",
   "metadata": {},
   "source": [
    "## Init new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cafe2af2-01eb-408a-b593-13bd46ffdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del optimizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = QuartzLM(in_features=N_MELS, hid_size=300, num_layers=2).to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "START_EPOCH = -1\n",
    "history = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6327f-410f-4d23-b7d8-3543e9da08a1",
   "metadata": {},
   "source": [
    "## Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b01c3d-3824-4178-addd-54e5e777144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = load_from_checkpoint('ASR_checkpoints/checkpoint_epoch_3.pt', DEVICE)\n",
    "model = check['model']\n",
    "model.register_backward_hook(backward_hook)\n",
    "\n",
    "optimizer = check['optimizer']\n",
    "scheduler = check['scheduler']\n",
    "history = check['history']\n",
    "\n",
    "START_EPOCH = check['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1252875-6b69-4afa-9e2b-de9c55262d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_collator = partial(LibriDataset.batch_collate, value_to_pad_tokens=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425a094-7116-4cd2-a25d-235752939e77",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448a3b5d-0b0d-4f11-be2d-4664a94e282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = set_split(\n",
    "    batch_size=BATCH_SIZE, train_shuffle=True, collator=batch_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f1213-b412-401f-8ff1-3344625f71c9",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d40116a-4583-4c88-8285-9270f77fb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dec = InferenceDecoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6f02de4-c6bc-4087-9ea9-bab7c29eff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = next(iter(data.dataloaders['val']))\n",
    "sample = s['input_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d31b5d9-0349-4741-bd8f-75b559f45496",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0 = inference_dec.beam_search_decoding(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b9138d7-b30c-40eb-8041-dccc293fbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = inference_dec.beam_search_decoding(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab41376b-d7ab-4a2a-9bd3-9edcc981d8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 143, 690,  ..., 376, 801, 773],\n",
       "        [  1, 143, 690,  ..., 376, 801, 773],\n",
       "        [  1, 143, 690,  ..., 114, 550, 773],\n",
       "        ...,\n",
       "        [  1, 143, 690,  ..., 643, 643, 898],\n",
       "        [  1, 143, 690,  ..., 643, 643, 898],\n",
       "        [  1, 143, 690,  ..., 643, 643, 898]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74f900c4-3e85-4666-b8cf-fb1e523da232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 143, 690,  ..., 957, 107, 532],\n",
       "        [  1, 865, 593,  ...,   5, 122, 928],\n",
       "        [  1, 143, 690,  ..., 643, 643, 898],\n",
       "        ...,\n",
       "        [  1, 143, 690,  ..., 643, 643, 898],\n",
       "        [  1, 143, 690,  ..., 376, 801, 773],\n",
       "        [  1, 143, 690,  ..., 550, 168, 865]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75f9d4-954d-4524-8b15-ab5049352939",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dec.greedy_decoding(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
