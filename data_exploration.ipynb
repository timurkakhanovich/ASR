{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles:\n",
    "* QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions: https://arxiv.org/abs/1910.10261\n",
    "* ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context: https://arxiv.org/abs/2005.03191 \n",
    "* Conformer: Convolution-augmented Transformer for Speech Recognition: https://arxiv.org/abs/2005.08100 \n",
    "* Speech2Text overview: https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/#open-speech-to-text-russian- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "Source: https://www.openslr.org/12/\n",
    "* development set, \"clean\" speech\n",
    "* development set, \"other\", more challenging, speech\n",
    "* test set, \"clean\" speech\n",
    "* test set, \"other\" speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "* python==3.8.10\n",
    "* torch==1.10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CLEAN_PATH = r\"LibriSpeech/train-clean-100/data/\"\n",
    "TEST_CLEAN_PATH = r\"LibriSpeech/test-clean/data/\"\n",
    "TEST_OTHER_PATH = r\"LibriSpeech/test-other/data/\"\n",
    "DEV_CLEAN_PATH = r\"LibriSpeech/dev-clean/data/\"\n",
    "DEV_OTHER_PATH = r\"LibriSpeech/dev-other/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_paths(set_path, limits=-1):\n",
    "    samples = glob(set_path + '**/*.flac', recursive=True)[:limits]\n",
    "\n",
    "    return map(\n",
    "        lambda x: (torchaudio.load(x)[0], x.split('/')[-1].split('.')[0]), \n",
    "        samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_labels(set_path):\n",
    "    labels_path = glob(set_path + '**/*.trans.txt', recursive=True)\n",
    "\n",
    "    labels = dict()\n",
    "    for label in labels_path:\n",
    "        with open(label, 'r') as fout:\n",
    "            for line in fout:\n",
    "                items = line.split()\n",
    "                labels[items[0]] = ' '.join(items[1:])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_CLEAN_SAMPLES = samples_paths(TRAIN_CLEAN_PATH)\n",
    "TEST_CLEAN_SAMPLES = samples_paths(TEST_CLEAN_PATH)\n",
    "TEST_OTHER_SAMPLES = samples_paths(TEST_OTHER_PATH)\n",
    "DEV_CLEAN_SAMPLES = samples_paths(DEV_CLEAN_PATH)\n",
    "DEV_OTHER_SAMPLES = samples_paths(DEV_OTHER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tensor_set(set_path, limits=-1):\n",
    "    \"\"\"\n",
    "    Function to group data with their labels within dataset.  \n",
    "\n",
    "    :params set_path: the whole path to the data folder.\n",
    "    :returns: dict:\n",
    "        * samples: tensor of all the loaded audiodata within dataset; \n",
    "        * labels: list of all text corresponding to the audio; \n",
    "        * mask: to identify the end of the signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading data in format (np.array of signal, text_id) -> list.  \n",
    "    samples = list(samples_paths(set_path, limits))\n",
    "    # Loading data in format {text_id: text} -> dict.  \n",
    "    labels = samples_labels(set_path)\n",
    "\n",
    "    # Signals in list form to represent them as a tensor with paddings.  \n",
    "    signals = [s[0] for s, _ in samples]\n",
    "    labels = [labels[l] for _, l in samples]\n",
    "    \n",
    "    signal_lengths = [len(s) for s in signals]\n",
    "    max_length = max(signal_lengths)\n",
    "    tail_to_pad = [max_length - l for l in signal_lengths]\n",
    "\n",
    "    padded_set = torch.vstack([\n",
    "        F.pad(x, pad=(0, l))\n",
    "        for x, l in zip(signals, tail_to_pad)\n",
    "    ])\n",
    "    mask = torch.zeros((len(samples), max_length))\n",
    "\n",
    "    for row in range(len(samples)):\n",
    "        mask[row, :tail_to_pad[row]] = 1\n",
    "\n",
    "    return {\n",
    "        'samples': padded_set, \n",
    "        'labels': labels, \n",
    "        'mask': mask.type(torch.bool)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gen_tensor_set(TEST_CLEAN_PATH, limits=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.6294e-04,  2.8381e-03, -3.9062e-03,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-6.4087e-04, -2.7466e-04,  1.5259e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 4.5776e-04,  5.4932e-04, -1.8311e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [-5.4932e-04, -7.0190e-04, -2.1362e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-9.4604e-04, -8.5449e-04, -5.7983e-04,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.5259e-04,  3.3569e-04,  9.1553e-05,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['samples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torchaudio.datasets.LIBRISPEECH(\"LibriSpeech/\", url='dev-clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3df1f12202395cfba320debf13bdbe0db0b66a14eed8247902390eb560721b75"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('speech2text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
