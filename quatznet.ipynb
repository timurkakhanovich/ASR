{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e59b7b-da04-4c7f-9f99-32f3a76cf572",
   "metadata": {},
   "source": [
    "# QuartzNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3dd86-a301-4fa4-aa13-16f02e7b1ccb",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "* NovoGrad optimizer;\n",
    "* Cosine Annealing scheduler;\n",
    "* Beam Search;\n",
    "* Delete repeating tokens;\n",
    "* Attention mask for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f501668-618a-4b81-b1ca-798c65984054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "from collections import OrderedDict, defaultdict\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ccbc4-b9b8-4bd4-a46b-3d05b5f884c3",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4d93d2-559c-4e49-b38b-23bf3acbb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRProcessor(object):\n",
    "    def __init__(self, sampling_rate=16000, n_fft=1024, hop_length=256, n_mels=64):\n",
    "        self.audio_preprocessor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        self.let2idx = [\" \", \"'\", \"<PAD>\"]\n",
    "        self.let2idx.extend([s for s in string.ascii_lowercase])\n",
    "        self.vocab = {w: idx for idx, w in enumerate(self.let2idx)}\n",
    "\n",
    "    def text_preprocessor(self, text):\n",
    "        sym_tokenize = [s for s in text]\n",
    "        \n",
    "        return torch.LongTensor([self.vocab[symbol] for symbol in sym_tokenize])\n",
    "    \n",
    "    def __call__(self, input_values: torch.tensor, labels: str) -> Dict[str, Union[torch.tensor, torch.LongTensor]]:\n",
    "        return {\n",
    "            'input_features': self.audio_preprocessor(input_values), \n",
    "            'labels': self.text_preprocessor(labels)\n",
    "        }\n",
    "    \n",
    "    def labels_decode(self, batched_labels):\n",
    "        pad_idx = len(processor.vocab) - 1\n",
    "        space_idx = len(processor.vocab) - 2\n",
    "        decoding_labels = batched_labels.clone().detach()\n",
    "        \n",
    "        decoding_labels[decoding_labels == pad_idx] = space_idx\n",
    "        return [''.join([self.let2idx[l] for l in bl]).strip() for bl in decoding_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4c3e-3638-4407-ae17-5a73c513391c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b379dd-8d43-4d5a-a2c1-81668acd9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, processor, root='', split='val', max_length=150):\n",
    "        assert split in ['dev-clean', 'dev-other', 'test-clean', 'test-other', 'train-clean-100'], \\\n",
    "                'Split error!'\n",
    "        \n",
    "        self.data_iterator = torchaudio.datasets.LIBRISPEECH(root=root, url=split)\n",
    "        \n",
    "        self.processor = processor\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_iterator[idx]\n",
    "        sample = self.processor(sample[0][0], sample[2].lower())\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_collate(batch):\n",
    "        # Collate audio samples.  \n",
    "        sample_tokens_lengths = torch.tensor([x['input_features'].size(1) for x in batch])\n",
    "        max_len_per_samples = torch.max(sample_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_samples % 2)\n",
    "        max_len_per_samples += additive\n",
    "        samples_lengths_to_pad = max_len_per_samples - sample_tokens_lengths - additive\n",
    "        \n",
    "        input_features = torch.stack([\n",
    "            F.pad(x['input_features'], pad=(0, val_to_pad)) \n",
    "            for x, val_to_pad in zip(batch, samples_lengths_to_pad)\n",
    "        ])\n",
    "        \n",
    "        # Collate label samples.  \n",
    "        label_tokens_lengths = torch.tensor([x['labels'].size(0) for x in batch])\n",
    "        max_len_per_labels = torch.max(label_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        additive = (max_len_per_labels % 2)\n",
    "        max_len_per_labels += additive\n",
    "        labels_lengths_to_pad = max_len_per_labels - label_tokens_lengths - additive\n",
    "\n",
    "        labels = torch.vstack([\n",
    "            F.pad(x['labels'], pad=(0, val_to_pad), value=len(processor.vocab) - 1) \n",
    "            for x, val_to_pad in zip(batch, labels_lengths_to_pad)\n",
    "        ]).type(torch.int64)\n",
    "\n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'input_lengths': torch.ceil(sample_tokens_lengths / 2).type(torch.int64), \n",
    "            'targets': labels, \n",
    "            'target_lengths': label_tokens_lengths\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0762fd1e-3492-4c09-9023-1d84d8b5b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_split(processor, batch_size=32, train_shuffle=False):\n",
    "    datasets = {\n",
    "        'train': LibriDataset(processor, split='train-clean-100'), \n",
    "        'val': LibriDataset(processor, split='dev-clean'), \n",
    "        'test': LibriDataset(processor, split='test-clean')\n",
    "    }\n",
    "    \n",
    "    dataloaders = {\n",
    "        k: DataLoader(datasets[k], batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=LibriDataset.batch_collate, num_workers=1)\n",
    "        for k in ['val', 'test']\n",
    "    }\n",
    "    \n",
    "    dataloaders['train'] = DataLoader(datasets['train'], batch_size=batch_size, shuffle=train_shuffle, \n",
    "                        collate_fn=LibriDataset.batch_collate, num_workers=1)\n",
    "    \n",
    "    return datasets, dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890add-9628-428c-a401-a0fac9e8e2f7",
   "metadata": {},
   "source": [
    "## Model (QuartzNet 5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e5e8ae-7a85-41da-aeff-a6eeb1cc5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, activation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Padding 'same'.  \n",
    "        padding = (kernel_size // 2) * dilation\n",
    "        \n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, kernel_size, stride, \n",
    "            padding=padding, dilation=dilation, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        TCS_out = self.pointwise(self.depthwise(x))\n",
    "        \n",
    "        bn_out = self.batch_norm(TCS_out)\n",
    "        \n",
    "        return F.relu(bn_out) if self.activation else bn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542c83a9-0a60-420d-b973-8aab23e4f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedBBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, R=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The first block to match in_channels and out_channels\n",
    "        self.B = [SingleBBlock(in_channels, out_channels, kernel_size, stride, dilation, activation=True)]\n",
    "        \n",
    "        # BBlocks between the first and the last blocks.  \n",
    "        self.B.extend([\n",
    "            SingleBBlock(out_channels, out_channels, kernel_size, stride, dilation, activation=True)\n",
    "            for _ in range(R - 2)\n",
    "        ])\n",
    "        \n",
    "        # The last block to prevent nonlinearity.  \n",
    "        self.B.append(SingleBBlock(out_channels, out_channels, kernel_size, stride, dilation, activation=False))\n",
    "        self.B = nn.Sequential(*self.B)\n",
    "        \n",
    "        # Skip connection.  \n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1), \n",
    "            nn.BatchNorm1d(num_features=out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        RBlocks_out = self.B(x)\n",
    "        skip_out = self.skip_connection(x)\n",
    "        \n",
    "        return F.relu(RBlocks_out + skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d816a157-69e4-4723-bb54-ddb0d7f3e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C1 = SingleBBlock(\n",
    "            in_channels=n_features, out_channels=256, kernel_size=33, \n",
    "            stride=2, dilation=1, activation=True\n",
    "        )\n",
    "        \n",
    "        self.B = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('B1', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=33, stride=1, dilation=1, R=5)), \n",
    "                ('B2', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=39, stride=1, dilation=1, R=5)), \n",
    "                ('B3', RepeatedBBlocks(in_channels=256, out_channels=512, kernel_size=51, stride=1, dilation=1, R=5)), \n",
    "                ('B4', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=63, stride=1, dilation=1, R=5)), \n",
    "                ('B5', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=75, stride=1, dilation=1, R=5))\n",
    "            ])\n",
    "        )\n",
    "        self.C2 = SingleBBlock(\n",
    "            in_channels=512, out_channels=512, kernel_size=87, stride=1, dilation=2, activation=True\n",
    "        )\n",
    "        \n",
    "        self.C3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=1, stride=1, dilation=1), \n",
    "            nn.BatchNorm1d(num_features=1024), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.C4 = nn.Conv1d(in_channels=1024, out_channels=n_classes, kernel_size=1, stride=1, dilation=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_conv_out = self.C1(x)\n",
    "\n",
    "        b_out = self.B(first_conv_out)\n",
    "        c2_out = self.C2(b_out)\n",
    "        c3_out = self.C3(c2_out)\n",
    "        c4_out = self.C4(c3_out)\n",
    "        \n",
    "        return c4_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942dfe0d-bec5-4caf-a88d-fadeed056e20",
   "metadata": {},
   "source": [
    "## Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1a6c1fa-0513-46d2-a62d-8963ba8b930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(checkpoint_path, device=torch.device('cpu')):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    model = checkpoint['model_architecture'].to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    optimizer = checkpoint['optimizer']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    history = checkpoint['whole_history']\n",
    "\n",
    "    if checkpoint['scheduler']:\n",
    "        scheduler = checkpoint['scheduler']\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    return {\n",
    "        'model': model, \n",
    "        'optimizer': optimizer, \n",
    "        'scheduler': scheduler if checkpoint['scheduler'] else None,\n",
    "        'history': history,\n",
    "        'epoch': epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767802-7f4f-4a9a-b4b7-f32cdd8ebc3c",
   "metadata": {},
   "source": [
    "## Metrics and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cfa558d-789b-40b1-8f57-219accc7deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wer_metric = load_metric('wer')\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward of labels prediction:\n",
    "        :param y_pred: log_softmax(output) of the model, shape: (T, B, C), \n",
    "        :param y_true: true labels, shape (B, T)\n",
    "        \"\"\"\n",
    "        pred_ids = torch.argmax(y_pred, dim=2).T\n",
    "    \n",
    "        pred_str = processor.labels_decode(pred_ids)\n",
    "        label_str = processor.labels_decode(y_true)\n",
    "\n",
    "        wer = self.wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "        return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07300da8-28ca-42bf-8195-2c1749e3d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_dataloader, criterion, metrics, \n",
    "                    device=torch.device('cpu'), return_train=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        print(\"\\n\")\n",
    "        for batch_idx, sample in enumerate(val_dataloader):\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(val_dataloader) - 1:\n",
    "                print(f\"==> Batch: {batch_idx}/{len(val_dataloader)}\")\n",
    "            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_score += metrics(sample['log_probs'], sample['targets'])\n",
    "\n",
    "        running_loss /= len(val_dataloader)\n",
    "        running_score /= len(val_dataloader)\n",
    "        \n",
    "    if return_train:\n",
    "        model.train()\n",
    "\n",
    "    return running_loss, running_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259e329-dece-444d-a5a9-4d3b118e1bc6",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1495fc5b-e82f-40f9-bdd1-13086658489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, criterion, optimizer, metrics, scheduler=None, \n",
    "          num_epochs=5, start_epoch=-1, prev_metrics=dict(), device=torch.device('cpu'),\n",
    "          folder_for_checkpoints='/'):\n",
    "    for key, vals in prev_metrics.items():\n",
    "        for val in vals:\n",
    "            mlflow.log_metric(key, val[1])\n",
    "\n",
    "    if len(prev_metrics) > 0:\n",
    "        history = copy.deepcopy(prev_metrics)\n",
    "        curr_step = prev_metrics['train_loss'][-1][0] + 1\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        curr_step = 1\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch + 1, start_epoch + 1 + num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "\n",
    "        clear_output(True)\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Epoch: {epoch}/{start_epoch + num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(\"Train: \")\n",
    "\n",
    "        for batch_idx, sample in enumerate(tqdm(dataloaders['train'])):            \n",
    "            sample = {k: v.to(device) for k, v in sample.items()}\n",
    "            \n",
    "            y_pred = model(sample['input_features'])\n",
    "            sample['log_probs'] = F.log_softmax(y_pred, dim=1).permute(2, 0, 1)\n",
    "            del sample['input_features']\n",
    "            \n",
    "            loss = criterion(**sample)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            running_score += metrics(sample['log_probs'], sample['targets'])\n",
    "\n",
    "            if batch_idx % 1001 == 1000:\n",
    "                val_loss, val_metrics = validate_model(model, dataloaders['val'], criterion, \n",
    "                                                        metrics, device, return_train=True)\n",
    "                \n",
    "                mlflow.log_metric('train_loss', running_loss / (batch_idx + 1))\n",
    "                mlflow.log_metric('val_loss', val_loss)\n",
    "                history['train_loss'].append((curr_step, running_loss / (batch_idx + 1)))\n",
    "                history['val_loss'].append((curr_step, val_loss))\n",
    "                \n",
    "                mlflow.log_metric('val_wer', val_metrics)\n",
    "                history['val_wer'].append((curr_step, val_metrics))\n",
    "                \n",
    "                mlflow.log_metric('train_wer', running_score / (batch_idx + 1))\n",
    "                history['train_wer'].append((curr_step, running_score / (batch_idx + 1)))\n",
    "\n",
    "            curr_step += 1\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "        print(\"Validate: \")\n",
    "        mean_val_loss, mean_val_score = validate_model(model, dataloaders['val'], criterion, \n",
    "                                                        metrics, device, return_train=True)\n",
    "\n",
    "        print(f'Mean val loss: {mean_val_loss}')\n",
    "        for metric, score in mean_val_score.items():\n",
    "            print(f'\\nMean {metric}: {score}')\n",
    "\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'batch_size_training': dataloaders['train'].batch_size,\n",
    "            'model_architecture': model,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler': scheduler if scheduler else None,\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'whole_history': history\n",
    "        }\n",
    "\n",
    "        torch.save(state, folder_for_checkpoints + f'checkpoint_epoch_{epoch + 1}.pt')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fd038-6a82-4251-80e1-101703731967",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "003ad6e1-0ea4-45b8-8543-a48ccf608899",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 3\n",
    "CHECKPOINT_PATH = \"ASR_checkpoints/\"\n",
    "N_MELS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01a41c-23b1-4242-aa3c-13b8070bacc5",
   "metadata": {},
   "source": [
    "## Init new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5486ef81-951f-4e2b-b3d8-0a18a339d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del optimizer\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "model = QuartzNet(n_features=N_MELS, n_classes=len(ASRProcessor().vocab) - 1)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=5e-2)\n",
    "\n",
    "START_EPOCH = -1\n",
    "history = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cb3fb-bce1-4b5d-b22b-9c549e765f10",
   "metadata": {},
   "source": [
    "## Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e399a-7789-49dd-a5b9-70580f3d5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = load_from_checkpoint('ASR_checkpoints/checkpoint_epoch_1.pt', DEVICE)\n",
    "model = check['model']\n",
    "\n",
    "optimizer = check['optimizer']\n",
    "scheduler = check['scheduler']\n",
    "history = check['history']\n",
    "\n",
    "START_EPOCH = check['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f022da5-5eb9-4d6e-b5ff-d9449e48ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ASRProcessor()\n",
    "metrics = Metrics()\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f0585-b391-470c-850c-4bab51e69b6b",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8b7eaa2-29c0-43cf-940a-c8763fee452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dataloaders = set_split(\n",
    "    processor, batch_size=BATCH_SIZE, train_shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2394b69-5869-4991-a040-da6cc0f12bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(model, dataloaders['val'], criterion, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b13f5-2b8f-4f00-a24e-f5d3494357c1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d9788-752d-4772-9598-12370bf60668",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('databricks')\n",
    "mlflow.set_experiment(\"/Users/timkakhanovich@gmail.com/ASR/QuartzNet\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"QuartzNet\"):\n",
    "    mlflow.set_tags({\n",
    "        'Python': '.'.join(map(str, sys.version_info[:3])), \n",
    "        'Device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU', \n",
    "        'Dataset': 'LibriSpeech'\n",
    "    })\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('lr', LR)\n",
    "    \n",
    "    _, _ = train(\n",
    "        model, dataloaders, criterion, optimizer, scheduler, metrics, num_epochs=NUM_EPOCHS, \n",
    "        start_epoch=START_EPOCH, prev_metrics=history, device=device, \n",
    "        folder_for_checkpoints=CHECKPOINT_PATH\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
