{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e59b7b-da04-4c7f-9f99-32f3a76cf572",
   "metadata": {},
   "source": [
    "# QuartzNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3f501668-618a-4b81-b1ca-798c65984054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f0df75f-8d69-45fe-a854-8f6dc83e46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ccbc4-b9b8-4bd4-a46b-3d05b5f884c3",
   "metadata": {},
   "source": [
    "## Data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4b4d93d2-559c-4e49-b38b-23bf3acbb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRProcessor(object):\n",
    "    def __init__(self, sampling_rate=16000, n_fft=1024, hop_length=512, n_mels=128):\n",
    "        self.audio_preprocessor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, \n",
    "            n_fft=n_fft, \n",
    "            hop_length=hop_length, \n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        \n",
    "        self.let2idx = [s for s in string.ascii_lowercase]\n",
    "        self.let2idx.extend([\"'\", \" \", \"<PAD>\"])\n",
    "        self.vocab = {w: idx for idx, w in enumerate(self.let2idx)}\n",
    "\n",
    "    def text_preprocessor(self, text):\n",
    "        sym_tokenize = [s for s in text]\n",
    "        \n",
    "        return torch.LongTensor([self.vocab[symbol] for symbol in sym_tokenize])\n",
    "    \n",
    "    def __call__(self, input_values: torch.tensor, labels: str) -> Dict[str, Union[torch.tensor, torch.LongTensor]]:\n",
    "        return {\n",
    "            'input_features': self.audio_preprocessor(input_values), \n",
    "            'labels': self.text_preprocessor(labels)\n",
    "        }\n",
    "    \n",
    "    def labels_decode(self, batched_labels):\n",
    "        pad_idx = len(processor.vocab) - 1\n",
    "        space_idx = len(processor.vocab) - 2\n",
    "        decoding_labels = batched_labels.clone().detach()\n",
    "        \n",
    "        decoding_labels[decoding_labels == pad_idx] = space_idx\n",
    "        return [''.join([self.let2idx[l] for l in bl]).strip() for bl in decoding_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4c3e-3638-4407-ae17-5a73c513391c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "58b379dd-8d43-4d5a-a2c1-81668acd9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, processor, root='', split='val', max_length=150):\n",
    "        assert split in ['dev-clean', 'dev-other', 'test-clean', 'test-other', 'train-clean-100'], \\\n",
    "                'Split error!'\n",
    "        \n",
    "        self.data_iterator = torchaudio.datasets.LIBRISPEECH(root=root, url=split)\n",
    "        \n",
    "        self.processor = processor\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_iterator[idx]\n",
    "        sample = self.processor(sample[0][0], sample[2].lower())\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_collate(batch):\n",
    "        # Collate audio samples.  \n",
    "        sample_tokens_lengths = torch.tensor([x['input_features'].size(1) for x in batch])\n",
    "        max_len_per_samples = torch.max(sample_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        max_len_per_samples += (max_len_per_samples % 2)\n",
    "        lengths_to_pad = max_len_per_samples - sample_tokens_lengths - 1\n",
    "        \n",
    "        input_features = torch.stack([\n",
    "            F.pad(x['input_features'], pad=(0, val_to_pad)) \n",
    "            for x, val_to_pad in zip(batch, lengths_to_pad)\n",
    "        ])\n",
    "        \n",
    "        # Collate label samples.  \n",
    "        label_tokens_lengths = torch.tensor([len(x['labels']) for x in batch])\n",
    "        max_len_per_labels = torch.max(label_tokens_lengths)\n",
    "        \n",
    "        # Extend to even max_len.  \n",
    "        max_len_per_labels += (max_len_per_labels % 2)\n",
    "        lengths_to_pad = max_len_per_labels - label_tokens_lengths - 1\n",
    "\n",
    "        labels = torch.vstack([\n",
    "            F.pad(x['labels'], pad=(0, val_to_pad), value=len(processor.vocab) - 1) \n",
    "            for x, val_to_pad in zip(batch, lengths_to_pad)\n",
    "        ]).type(torch.int64)\n",
    "\n",
    "        return {\n",
    "            'input_features': input_features, \n",
    "            'input_lengths': sample_tokens_lengths, \n",
    "            'labels': labels, \n",
    "            'labels_lengths': label_tokens_lengths\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2eed184f-591e-4aca-8b9e-cb1ae984d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ASRProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "5651554b-e5aa-485b-8508-1008f92de7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': LibriDataset(processor, split='train-clean-100'), \n",
    "    'val': LibriDataset(processor, split='dev-clean')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "84d273c1-5731-4377-a376-b2b3477deb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    k: DataLoader(dataset[k], batch_size=BATCH_SIZE, shuffle=True, \n",
    "                    collate_fn=LibriDataset.batch_collate, num_workers=1)\n",
    "    for k in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b890add-9628-428c-a401-a0fac9e8e2f7",
   "metadata": {},
   "source": [
    "## Model (QuartzNet 5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "d2e5e8ae-7a85-41da-aeff-a6eeb1cc5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, kernel_size, \n",
    "            padding=padding, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        TCS_out = self.pointwise(self.depthwise(x))\n",
    "        \n",
    "        bn_out = self.batch_norm(TCS_out)\n",
    "        \n",
    "        return F.relu(bn_out) if self.activation else bn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "542c83a9-0a60-420d-b973-8aab23e4f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedBBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, R):\n",
    "        super().__init__()\n",
    "        # The first block to match in_channels and out_channels\n",
    "        self.B = [SingleBBlock(in_channels, out_channels, kernel_size, activation=True)]\n",
    "        \n",
    "        # BBlocks between the first and the last blocks.  \n",
    "        self.B.extend([\n",
    "            SingleBBlock(out_channels, out_channels, kernel_size, activation=True)\n",
    "            for _ in range(R - 2)\n",
    "        ])\n",
    "        \n",
    "        # The last block to prevent nonlinearity.  \n",
    "        self.B.append(SingleBBlock(out_channels, out_channels, kernel_size, activation=False))\n",
    "        self.B = nn.Sequential(*self.B)\n",
    "        \n",
    "        # Skip connection.  \n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1), \n",
    "            nn.BatchNorm1d(num_features=out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        RBlocks_out = self.B(x)\n",
    "        skip_out = self.skip_connection(x)\n",
    "        \n",
    "        return F.relu(RBlocks_out + skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d816a157-69e4-4723-bb54-ddb0d7f3e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_features, out_channels=256, kernel_size=33, stride=2), \n",
    "            nn.BatchNorm1d(num_features=256), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.B = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('B1', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=33, R=5)), \n",
    "                ('B2', RepeatedBBlocks(in_channels=256, out_channels=256, kernel_size=39, R=5)), \n",
    "                ('B3', RepeatedBBlocks(in_channels=256, out_channels=512, kernel_size=51, R=5)), \n",
    "                ('B4', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=63, R=5)), \n",
    "                ('B5', RepeatedBBlocks(in_channels=512, out_channels=512, kernel_size=75, R=5))\n",
    "            ])\n",
    "        )\n",
    "        self.C2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=512, kernel_size=87, padding=43), \n",
    "            nn.BatchNorm1d(num_features=512), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.C3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=1), \n",
    "            nn.BatchNorm1d(num_features=1024), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.C4 = nn.Conv1d(in_channels=1024, out_channels=n_classes, kernel_size=1, dilation=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_conv_out = self.C1(x)\n",
    "        b_out = self.B(first_conv_out)\n",
    "        c2_out = self.C2(b_out)\n",
    "        c3_out = self.C3(c2_out)\n",
    "        c4_out = self.C4(c3_out)\n",
    "        \n",
    "        return c4_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "fa4bec29-1f3c-42de-817f-03f295072510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_it = iter(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c05e2165-6ec5-41b4-9f48-ce3c3cbb31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(data_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "6c54cb35-89dd-4cca-bca7-42621ee0e874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 509])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f4673851-a4a9-47b4-ab78-294b7bd96924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([260, 426, 260, 355, 429, 472, 391, 471, 475, 451, 451, 244, 351, 496,\n",
       "        456,  71, 441, 463, 503, 462, 163, 316, 456, 460, 509, 388, 377, 469,\n",
       "        501, 491, 178, 497])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "33aa0de4-1cfe-4312-bf14-2a1aebf3fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuartzNet(n_features=128, n_classes=len(ASRProcessor().vocab) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "5cac12e7-d97d-412c-a770-4acb65e02876",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(sample['input_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "36c0028b-2f18-41f7-b088-9b7a9c346431",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "491b7a0d-b158-425c-9a1c-39dfa050dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 25, 26,  ..., 25, 11, 26],\n",
       "        [16, 10,  6,  ..., 10, 24, 10],\n",
       "        [24, 13, 26,  ..., 10, 10, 24],\n",
       "        ...,\n",
       "        [19, 26, 26,  ..., 10, 10, 10],\n",
       "        [ 7, 10, 10,  ..., 24, 10, 10],\n",
       "        [17, 19,  6,  ..., 24, 11, 10]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767802-7f4f-4a9a-b4b7-f32cdd8ebc3c",
   "metadata": {},
   "source": [
    "## Metrics and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "58500ca9-5e46-4fbc-a9f5-48d9dc873a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric('wer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "24bf5377-dfa5-46cb-96bc-e63289ee168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred_logits, y_true):\n",
    "    pred_ids = torch.argmax(y_pred_logits, dim=1)\n",
    "    \n",
    "    pred_str = processor.labels_decode(pred_ids)\n",
    "    label_str = processor.labels_decode(y_true)\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f55e1fc6-2f35-4bb8-bee3-2b3a4545174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_for_loss = torch.log(logits).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a6f3da1c-113a-46b8-9290-afda0445de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CTCLoss(blank=len(processor.vocab) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dc9c68d6-b310-4b4f-a5ff-aaa6ca7b39ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits, sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a061462e-3d64-4a5c-b580-fa5fcedb7120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([246, 32, 28])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_for_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "006f3942-0189-4f4f-b714-c4b4ef7b4ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 523])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "41fcf23e-8666-46b4-b22b-4fa7c8627ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size at least 284 at dimension 1, but got size 283 for argument #2 'targets' (while checking arguments for ctc_loss_cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_for_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_lengths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_lengths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/speech2text/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/speech2text/lib/python3.8/site-packages/torch/nn/modules/loss.py:1714\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/speech2text/lib/python3.8/site-packages/torch/nn/functional.py:2460\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2455\u001b[0m         ctc_loss,\n\u001b[1;32m   2456\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2457\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2458\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[1;32m   2459\u001b[0m     )\n\u001b[0;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[1;32m   2462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size at least 284 at dimension 1, but got size 283 for argument #2 'targets' (while checking arguments for ctc_loss_cpu)"
     ]
    }
   ],
   "source": [
    "loss(logits_for_loss, sample['labels'], sample['input_lengths'], sample['labels_lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "66de9dd8-15f5-4b54-a561-d0378b8ec84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 283])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cd00bfcc-e2b6-4a52-a9ab-1f9d782afb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 293])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_it)['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7961a-f52d-442b-90a8-0b10186c4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_dataloader, criterion, metrics, \n",
    "                    device=torch.device('cpu'), return_train=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    predictions = torch.FloatTensor([]).to(device)\n",
    "    targets = torch.FloatTensor([]).to(device)\n",
    "    with torch.inference_mode():\n",
    "        print(\"\\n\")\n",
    "        for batch_idx, sample in enumerate(val_dataloader):\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(val_dataloader) - 1:\n",
    "                print(f\"==> Batch: {batch_idx}/{len(val_dataloader)}\")\n",
    "            \n",
    "            X = sample['input_features'].to(device)\n",
    "            y_true = sample['labels'].to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            logits = F.log_softmax(y_pred)\n",
    "            \n",
    "            loss = criterion(logits, y_true)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predictions = torch.cat((predictions, y_pred))\n",
    "            targets = torch.cat((targets, y_true))\n",
    "\n",
    "        all_metrics_score = metrics(predictions, targets)\n",
    "        running_loss /= len(val_dataloader)\n",
    "        \n",
    "    if return_train:\n",
    "        model.train()\n",
    "\n",
    "    return running_loss, all_metrics_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843cc25-8a0c-4873-92bf-f48b1f8582f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
